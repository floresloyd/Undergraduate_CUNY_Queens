X_train = diamonds_sample %>% select(-price)
# Define range for mtry
max_mtry = ncol(X_train)
mtry_values = 1:max_mtry
oob_se_by_mtry = numeric(length(mtry_values))
# Build model and calculate oob se
for (i in seq_along(mtry_values)) {
rf_mod <- randomForest(X_train, y_train, ntree = 500, mtry = mtry_values[i], keep.inbag = TRUE)
e_oob <- y_train - rf_mod$predicted
oob_se_by_mtry[i] <- sd(e_oob, na.rm = TRUE) / sqrt(sum(!is.na(e_oob)))
}
#TO-DO
results_df <- data.frame(
mtry = mtry_values,
oob_se = oob_se_by_mtry
)
# Plotting
ggplot(results_df, aes(x = mtry, y = oob_se)) +
geom_line() +
geom_point(shape = 21, fill = "blue") +
labs(title = "OOB Standard Error by mtry Values",
x = "mtry (Number of Variables Tried at Each Split)",
y = "OOB Standard Error") +
theme_minimal()
rm(list = ls())
set.seed(1)
pacman::p_load_gh("coatless/ucidata")
adult_train = adult %>%
sample_n(2000)
adult_train = missForest(adult_train)$ximp
num_trees_values = c(1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000)
oob_se_bagged_trees_mod_by_num_trees = array(NA, length(num_trees_values))
#TO-DO
for (i in seq_along(num_trees_values)) {
set.seed(1)  # for reproducibility
rf_model = randomForest(income ~ ., data = adult_sample,
mtry = sqrt(ncol(adult_sample) - 1),
ntree = num_trees_values[i],
importance = TRUE,
do.trace = 100,
keep.forest = TRUE,
replace = TRUE)
oob_se_bagged_trees_mod_by_num_trees[i] = rf_model$err.rate[ntree(rf_model), "OOB"]
}
data("adult")
adult_sample = adult %>%
sample_n(2000)
num_trees_values = c(1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000)
oob_se_bagged_trees_mod_by_num_trees = array(NA, length(num_trees_values))
#TO-DO
for (i in seq_along(num_trees_values)) {
set.seed(1)  # for reproducibility
rf_model = randomForest(income ~ ., data = adult_sample,
mtry = sqrt(ncol(adult_sample) - 1),
ntree = num_trees_values[i],
importance = TRUE,
do.trace = 100,
keep.forest = TRUE,
replace = TRUE)
oob_se_bagged_trees_mod_by_num_trees[i] = rf_model$err.rate[ntree(rf_model), "OOB"]
}
data("adult")
pacman::p_load(dplyr, randomForest, missForest, ucidata)
adult_sample = adult %>%
sample_n(2000)
num_trees_values = c(1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000)
oob_se_bagged_trees_mod_by_num_trees = array(NA, length(num_trees_values))
#TO-DO
for (i in seq_along(num_trees_values)) {
set.seed(1)  # for reproducibility
rf_model = randomForest(income ~ ., data = adult_sample,
mtry = sqrt(ncol(adult_sample) - 1),
ntree = num_trees_values[i],
importance = TRUE,
do.trace = 100,
keep.forest = TRUE,
replace = TRUE)
oob_se_bagged_trees_mod_by_num_trees[i] = rf_model$err.rate[ntree(rf_model), "OOB"]
}
data("adult")
pacman::p_load(dplyr, randomForest, missForest, ucidata)
adult_sample = adult %>%
sample_n(2000)
adult_sample = missForest(adult_sample)$ximp
num_trees_values = c(1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000)
oob_se_bagged_trees_mod_by_num_trees = array(NA, length(num_trees_values))
#TO-DO
for (i in seq_along(num_trees_values)) {
set.seed(1)  # for reproducibility
rf_model = randomForest(income ~ ., data = adult_sample,
mtry = sqrt(ncol(adult_sample) - 1),
ntree = num_trees_values[i],
importance = TRUE,
do.trace = 100,
keep.forest = TRUE,
replace = TRUE)
oob_se_bagged_trees_mod_by_num_trees[i] = rf_model$err.rate[ntree(rf_model), "OOB"]
}
rm(list = ls())
set.seed(1)
pacman::p_load_gh("coatless/ucidata")
pacman::p_load(dplyr, randomForest, missForest, ucidata)
data("adult")
adult_sample = adult %>%
sample_n(2000)
# Impute missing values
adult_sample = missForest(adult_sample)$ximp
num_trees_values = c(1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000)
# Initialize a vector to store OOB errors
oob_se_bagged_trees_mod_by_num_trees = numeric(length(num_trees_values))
# Loop over the number of trees, fit the model, and store the OOB error
for (i in seq_along(num_trees_values)) {
set.seed(1)  # for reproducibility
rf_model = randomForest(income ~ ., data = adult_sample,
mtry = sqrt(ncol(adult_sample) - 1),
ntree = num_trees_values[i],
importance = TRUE,
do.trace = 100,
keep.forest = TRUE,
replace = TRUE)
oob_se_bagged_trees_mod_by_num_trees[i] = rf_model$err.rate[ntree(rf_model), "OOB"]
}
rm(list = ls())
set.seed(1)
pacman::p_load_gh("coatless/ucidata")
pacman::p_load(dplyr, randomForest, missForest, ucidata)
data("adult")
adult_sample = adult %>%
sample_n(2000)
# Impute missing values
adult_sample = missForest(adult_sample)$ximp
num_trees_values = c(1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000)
# Initialize a vector to store OOB errors
oob_se_bagged_trees_mod_by_num_trees = numeric(length(num_trees_values))
# Loop over the number of trees, fit the model, and store the OOB error
for (i in seq_along(num_trees_values)) {
set.seed(1)  # for reproducibility
rf_model = randomForest(income ~ ., data = adult_sample,
mtry = sqrt(ncol(adult_sample) - 1),
ntree = num_trees_values[i],
importance = TRUE,
do.trace = 100,
keep.forest = TRUE,
replace = TRUE)
oob_se_bagged_trees_mod_by_num_trees[i] = rf_model$err.rate[rf_model$ntree, "OOB"]
}
# Plot the OOB error against the number of trees
plot(num_trees_values, oob_se_bagged_trees_mod_by_num_trees, type = "b",
xlab = "Number of Trees", ylab = "OOB Error Rate",
main = "OOB Error Rate vs. Number of Trees in Bagged Trees Model")
oob_se_rf_mod_by_num_trees = array(NA, length(num_trees_values))
#TO-DO
for (i in seq_along(num_trees_values)) {
set.seed(1)  # for reproducibility
rf_model = randomForest(income ~ ., data = adult_sample,
mtry = sqrt(ncol(adult_sample) - 1),
ntree = num_trees_values[i],
importance = TRUE,
do.trace = 100,
keep.forest = TRUE,
replace = TRUE)
# Access OOB error rate directly from model object
oob_se_rf_mod_by_num_trees[i] = mean(rf_model$oob.error)
}
cbind(
num_trees_values,
(oob_se_rf_mod_by_num_trees - oob_se_bagged_trees_mod_by_num_trees) / oob_se_bagged_trees_mod_by_num_trees * 100
)
oob_se_by_mtry = array(NA, ncol(adult_train))
oob_se_by_mtry = array(NA, ncol(adult))
#TO-DO
# Get the maximum number of features for mtry
max_mtry = ncol(adult_train) - 1
oob_se_by_mtry = array(NA, ncol(adult_train))
#TO-DO+
data("adult")
# Split data into training and testing sets (replace 0.75 with your desired split ratio)
split = sample(1:nrow(adult), size = nrow(adult) * 0.75)
adult_train = adult[split, ]
adult_test = adult[-split, ]
# Impute missing values using missForest
adult_train = missForest(adult_train)$ximp
# Define maximum mtry
max_mtry = ncol(adult_train) - 1
# OOB error for different mtry values (500 trees)
oob_se_by_mtry = array(NA, max_mtry)
for (m in 1:max_mtry) {
set.seed(1)
rf_model = randomForest(income ~ ., data = adult_train,
mtry = m,
ntree = 500,
importance = TRUE,
do.trace = 100,
keep.forest = TRUE,
replace = TRUE)
oob_se_by_mtry[m] = mean(rf_model$oob.error)
}
# OOB error for different number of trees (mtry = sqrt(ncol(adult_train) - 1))
num_trees_values = c(1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000)
oob_se_rf_mod_by_num_trees = array(NA, length(num_trees_values))
for (i in seq_along(num_trees_values)) {
set.seed(1)
rf_model = randomForest(income ~ ., data = adult_train,
mtry = sqrt(ncol(adult_train) - 1),
ntree = num_trees_values[i],
importance = TRUE,
do.trace = 100,
keep.forest = TRUE,
replace = TRUE)
oob_se_rf_mod_by_num_trees[i] = mean(rf_model$oob.error)
}
# Now you can use oob_se_by_mtry and oob_se_rf_mod_by_num_trees for further analysis
# Example: Plot OOB error vs mtry
plot(1:max_mtry, oob_se_by_mtry, type = "b",
xlab = "mtry", ylab = "OOB Error Rate",
main = "OOB Error Rate vs. mtry in Random Forest Model")
#TO-DO
# Example: Plot OOB error vs mtry
plot(1:max_mtry, oob_se_by_mtry, type = "b",
xlab = "mtry", ylab = "OOB Error Rate",
main = "OOB Error Rate vs. mtry in Random Forest Model")
#TO-DO
plot(1:max_mtry, oob_se_by_mtry, type = "b",
xlab = "mtry", ylab = "OOB Error Rate",
main = "OOB Error Rate vs. mtry in Random Forest Model")
# Check for missing values
if (any(is.na(oob_se_by_mtry))) {
# Remove missing values (if any)
oob_se_by_mtry = oob_se_by_mtry[!is.na(oob_se_by_mtry)]
warning("Missing values found in oob_se_by_mtry, removed for plotting.")
}
# Explore OOB error rates (optional)
summary(oob_se_by_mtry)  # Check for outliers or extreme values
# Plot OOB error vs mtry (assuming finite values now)
plot(1:max_mtry, oob_se_by_mtry, type = "b",
xlab = "mtry", ylab = "OOB Error Rate",
main = "OOB Error Rate vs. mtry in Random Forest Model")
# Calculate proportion of each class in training data
prop_positive <- sum(adult_train$income == ">50K") / nrow(adult_train)
prop_negative <- 1 - prop_positive
# Calculate weighted OOB error with costs
oob_total_cost <- mean(
rf_model$oob.error * (cost_fp * prop_positive + cost_fn * prop_negative)
)
rm(list = setdiff(ls(), "adult_train"))
rm(list = setdiff(ls(), "adult_train"))
#TO-DO
library(caret)  # for data splitting and preprocessing
library(mice)   # for missing data imputation
rm(list = setdiff(ls(), "adult_train"))
#TO-DO
library(caret)  # for data splitting and preprocessing
pacman::p_load_gh("mice")
library(nnet)   # for multinom function to fit logistic regression
library(pROC)   # for ROC curve
library(ROCR)   # for performance metrics
pacman::p_load("ROCR")
rm(list = setdiff(ls(), "adult_train"))
#TO-DO
library(caret)  # for data splitting and preprocessing
pacman::p_load_gh("mice")
library(nnet)   # for multinom function to fit logistic regression
library(pROC)   # for ROC curve
pacman::p_load("ROCR")
imputed_data <- mice(adult_train, m=5, method='pmm', seed=500)
rm(list = setdiff(ls(), "adult_train"))
#TO-DO
library(caret)  # for data splitting and preprocessing
pacman::p_load("mice")
library(nnet)   # for multinom function to fit logistic regression
library(pROC)   # for ROC curve
pacman::p_load("ROCR")
imputed_data <- mice(adult_train, m=5, method='pmm', seed=500)
completed_data <- complete(imputed_data)
# Define the model - assuming 'income' is the target variable
# Adjust the formula as per your dataset
logit_model <- glm(income ~ ., data = completed_data, family = binomial())
# Summary of the model to check coefficients and model validity
summary(logit_model)
# Predict probabilities
probabilities <- predict(logit_model, type = "response")
# ROC Curve
roc_curve <- roc(completed_data$income, probabilities)
plot(roc_curve, main="ROC Curve")
# AUC Score
auc(roc_curve)
# DET Curve using ROCR
pred <- prediction(probabilities, completed_data$income)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize = TRUE, main="DET Curve")
pacman::p_load(tidyverse)
asymmetric_predictions_results = tibble(
p_hat_threshold = seq(from = 0.001, to = 0.999, by = 0.001),
misclassification_error = NA,
precision = NA,
recall = NA,
F1 = NA,
FDR = NA,
FOR = NA
)
#TO-DO
probabilities <- predict(logit_model, completed_data, type = "response")
# Define the target variable
actual <- completed_data$income
# Function to calculate metrics
calculate_metrics <- function(threshold, actual, predicted) {
prediction <- ifelse(predicted >= threshold, 1, 0)
cm <- confusionMatrix(as.factor(prediction), as.factor(actual), positive = "1")
data.frame(
misclassification_error = 1 - cm$overall['Accuracy'],
precision = cm$byClass['Precision'],
recall = cm$byClass['Sensitivity'],
F1 = 2 * (cm$byClass['Precision'] * cm$byClass['Sensitivity']) / (cm$byClass['Precision'] + cm$byClass['Sensitivity']),
FDR = 1 - cm$byClass['Precision'],
FOR = 1 - cm$byClass['Negative Predictive Value']
)
}
# Create the tibble and calculate metrics for each threshold
asymmetric_predictions_results <- tibble(
p_hat_threshold = seq(from = 0.001, to = 0.999, by = 0.001)
) %>%
mutate(metrics = map(p_hat_threshold, calculate_metrics, actual = actual, predicted = probabilities)) %>%
unnest(metrics)
pacman::p_load(tidyverse)
asymmetric_predictions_results = tibble(
p_hat_threshold = seq(from = 0.001, to = 0.999, by = 0.001),
misclassification_error = NA,
precision = NA,
recall = NA,
F1 = NA,
FDR = NA,
FOR = NA
)
#TO-DO
probabilities <- predict(logit_model, completed_data, type = "response")
# Define the target variable
actual <- completed_data$income
# Function to calculate metrics
calculate_metrics <- function(threshold, actual, predicted) {
prediction <- ifelse(predicted >= threshold, 1, 0)
cm <- confusionMatrix(as.factor(prediction), as.factor(actual), positive = "1")
data.frame(
misclassification_error = 1 - cm$overall['Accuracy'],
precision = cm$byClass['Precision'],
recall = cm$byClass['Sensitivity'],
F1 = 2 * (cm$byClass['Precision'] * cm$byClass['Sensitivity']) / (cm$byClass['Precision'] + cm$byClass['Sensitivity']),
FDR = 1 - cm$byClass['Precision'],
FOR = 1 - cm$byClass['Negative Predictive Value']
)
}
# Create the tibble and calculate metrics for each threshold
asymmetric_predictions_results <- tibble(
p_hat_threshold = seq(from = 0.001, to = 0.999, by = 0.001)
) %>%
mutate(metrics = map(p_hat_threshold, calculate_metrics, actual = actual, predicted = probabilities)) %>%
unnest(metrics)
pacman::p_load(tidyverse)
asymmetric_predictions_results = tibble(
p_hat_threshold = seq(from = 0.001, to = 0.999, by = 0.001),
misclassification_error = NA,
precision = NA,
recall = NA,
F1 = NA,
FDR = NA,
FOR = NA
)
# Predict probabilities
probabilities <- predict(logit_model, completed_data, type = "response")
# Ensure the actual target variable is a factor with levels 0 and 1
actual <- factor(completed_data$income, levels = c("0", "1"))
# Function to calculate metrics
calculate_metrics <- function(threshold, actual, predicted) {
# Convert predicted probabilities to binary predictions based on the threshold
prediction <- ifelse(predicted >= threshold, "1", "0")
prediction <- factor(prediction, levels = c("0", "1"))
# Only compute metrics if there are both positive and negative predictions
if (all(levels(prediction) %in% levels(actual))) {
cm <- confusionMatrix(prediction, actual, positive = "1")
return(data.frame(
misclassification_error = 1 - cm$overall['Accuracy'],
precision = cm$byClass['Precision'],
recall = cm$byClass['Sensitivity'],
F1 = 2 * (cm$byClass['Precision'] * cm$byClass['Sensitivity']) / (cm$byClass['Precision'] + cm$byClass['Sensitivity']),
FDR = 1 - cm$byClass['Precision'],
FOR = 1 - cm$byClass['Negative Predictive Value']
))
} else {
return(data.frame(
misclassification_error = NA,
precision = NA,
recall = NA,
F1 = NA,
FDR = NA,
FOR = NA
))
}
}
# Create the tibble and calculate metrics for each threshold
asymmetric_predictions_results <- tibble(
p_hat_threshold = seq(from = 0.001, to = 0.999, by = 0.001)
) %>%
mutate(metrics = map(p_hat_threshold, calculate_metrics, actual = actual, predicted = probabilities)) %>%
unnest(metrics)
# View the results
print(asymmetric_predictions_results)
#TO-DO
cost_per_misclassification = 1
cost_per_fdr = 2
cost_per_for = 1.5
asymmetric_predictions_results <- asymmetric_predictions_results %>%
mutate(total_cost = misclassification_error * cost_per_misclassification +
FDR * cost_per_fdr +
FOR * cost_per_for)
# View the updated results
print(asymmetric_predictions_results)
#TO-DO
# Find the row with the minimum total cost
min_cost_data <- asymmetric_predictions_results %>%
filter(total_cost == min(total_cost, na.rm = TRUE)) %>%
slice(1)  # In case there are multiple minima, take the first
# Display the result
print(min_cost_data)
#TO-DO
roc_curve <- roc(response = completed_data$income, predictor = probabilities)
plot(roc_curve, main = "ROC Curve", col = "#1c61b6", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")  # Adding a diagonal reference line
#TO-DO
auc_value <- auc(roc_curve)
print(paste("AUC value:", auc_value))
#TO-DOpred <- prediction(probabilities, actual)
perf <- performance(pred, "fnr", "fpr")
# Plot the DET curve
plot(perf, colorize = TRUE)
abline(0, 1, lty = 2, col = "red")  # Adding a reference line
# Transforming the axis to normal deviate scale might require additional steps.
pacman::p_load(tidyverse)
asymmetric_predictions_results = tibble(
p_hat_threshold = seq(from = 0.001, to = 0.999, by = 0.001),
misclassification_error = NA,
precision = NA,
recall = NA,
F1 = NA,
FDR = NA,
FOR = NA
)
# Predict probabilities
probabilities <- predict(logit_model, completed_data, type = "response")
# Ensure the actual target variable is a factor with levels 0 and 1
actual <- factor(completed_data$income, levels = c("0", "1"))
# Function to calculate metrics
calculate_metrics <- function(threshold, actual, predicted) {
# Convert predicted probabilities to binary predictions based on the threshold
prediction <- ifelse(predicted >= threshold, "1", "0")
prediction <- factor(prediction, levels = c("0", "1"))
# Only compute metrics if there are both positive and negative predictions
if (all(levels(prediction) %in% levels(actual))) {
cm <- confusionMatrix(prediction, actual, positive = "1")
return(data.frame(
misclassification_error = 1 - cm$overall['Accuracy'],
precision = cm$byClass['Precision'],
recall = cm$byClass['Sensitivity'],
F1 = 2 * (cm$byClass['Precision'] * cm$byClass['Sensitivity']) / (cm$byClass['Precision'] + cm$byClass['Sensitivity']),
FDR = 1 - cm$byClass['Precision'],
FOR = 1 - cm$byClass['Negative Predictive Value']
))
} else {
return(data.frame(
misclassification_error = NA,
precision = NA,
recall = NA,
F1 = NA,
FDR = NA,
FOR = NA
))
}
}
# Create the tibble and calculate metrics for each threshold
asymmetric_predictions_results <- tibble(
p_hat_threshold = seq(from = 0.001, to = 0.999, by = 0.001)
) %>%
mutate(metrics = map(p_hat_threshold, calculate_metrics, actual = actual, predicted = probabilities)) %>%
unnest(metrics)
# View the results
print(asymmetric_predictions_results)
#TO-DO
cost_per_misclassification = 1
cost_per_fdr = 2
cost_per_for = 1.5
asymmetric_predictions_res <- asymmetric_predictions_results %>%
mutate(total_cost = misclassification_error * cost_per_misclassification +
FDR * cost_per_fdr +
FOR * cost_per_for)
# View the updated results
print(asymmetric_predictions_results)
#TO-DO
cost_per_misclassification = 1
cost_per_fdr = 2
cost_per_for = 1.5
asymmetric_predictions_results <- asymmetric_predictions_results %>%
mutate(total_cost = misclassification_error * cost_per_misclassification +
FDR * cost_per_fdr +
FOR * cost_per_for)
# View the updated results
print(asymmetric_predictions_results)
#TO-DO+
data("adult")
# Split data into training and testing sets (replace 0.75 with your desired split ratio)
split = sample(1:nrow(adult), size = nrow(adult) * 0.75)
adult_train = adult[split, ]
adult_test = adult[-split, ]
# Impute missing values using missForest
adult_train = missForest(adult_train)$ximp
